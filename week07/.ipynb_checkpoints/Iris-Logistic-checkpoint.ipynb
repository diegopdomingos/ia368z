{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from itertools import count\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2222  0.0678\n",
      " 0.1667  0.0678\n",
      " 0.1111  0.0508\n",
      " 0.0833  0.0847\n",
      " 0.1944  0.0678\n",
      "[torch.DoubleTensor of size 5x2]\n",
      "\n",
      "\n",
      " 1  0  0\n",
      " 1  0  0\n",
      " 1  0  0\n",
      " 1  0  0\n",
      " 1  0  0\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:,::2]  # comprimento das sépalas e pétalas, indices 0 e 2\n",
    "Xc = X - X.min(axis=0)\n",
    "Xc /= Xc.max(axis=0)\n",
    "\n",
    "y = torch.from_numpy(iris.target)\n",
    "Xc = torch.from_numpy(Xc)\n",
    "#y = torch.from_numpy(y_o.reshape(150,1))\n",
    "#y_oh = torch.zeros(y.size(0), 3).scatter_(1,y,1)\n",
    "\n",
    "print(Xc[0:5])\n",
    "print(y_oh[0:5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,y,epoch,b=10):\n",
    "    model.train()\n",
    "    random = np.random.randint(0,len(X),size=(1,b))\n",
    "    data, target = Variable(X[torch.LongTensor(random[0])].float()), Variable(y[torch.LongTensor(random[0])])\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, len(data), len(X),100. *len(data) / len(X), loss.data[0]))\n",
    "\n",
    "def test(X,y):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data, target = Variable(X.float(), volatile=True), Variable(y)\n",
    "    output = model(data)\n",
    "    test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [10/150 (7%)]\tLoss: 1.222092\n",
      "Train Epoch: 2 [10/150 (7%)]\tLoss: 1.215972\n",
      "Train Epoch: 3 [10/150 (7%)]\tLoss: 1.080959\n",
      "Train Epoch: 4 [10/150 (7%)]\tLoss: 1.350456\n",
      "Train Epoch: 5 [10/150 (7%)]\tLoss: 1.287712\n",
      "Train Epoch: 6 [10/150 (7%)]\tLoss: 1.149709\n",
      "Train Epoch: 7 [10/150 (7%)]\tLoss: 1.159784\n",
      "Train Epoch: 8 [10/150 (7%)]\tLoss: 1.227147\n",
      "Train Epoch: 9 [10/150 (7%)]\tLoss: 1.297720\n",
      "Train Epoch: 10 [10/150 (7%)]\tLoss: 1.084436\n",
      "Train Epoch: 11 [10/150 (7%)]\tLoss: 1.140369\n",
      "Train Epoch: 12 [10/150 (7%)]\tLoss: 1.090051\n",
      "Train Epoch: 13 [10/150 (7%)]\tLoss: 1.231349\n",
      "Train Epoch: 14 [10/150 (7%)]\tLoss: 1.023225\n",
      "Train Epoch: 15 [10/150 (7%)]\tLoss: 0.944725\n",
      "Train Epoch: 16 [10/150 (7%)]\tLoss: 1.292706\n",
      "Train Epoch: 17 [10/150 (7%)]\tLoss: 1.160374\n",
      "Train Epoch: 18 [10/150 (7%)]\tLoss: 1.222760\n",
      "Train Epoch: 19 [10/150 (7%)]\tLoss: 1.346080\n",
      "Train Epoch: 20 [10/150 (7%)]\tLoss: 1.156570\n",
      "Train Epoch: 21 [10/150 (7%)]\tLoss: 1.025408\n",
      "Train Epoch: 22 [10/150 (7%)]\tLoss: 1.167822\n",
      "Train Epoch: 23 [10/150 (7%)]\tLoss: 1.076361\n",
      "Train Epoch: 24 [10/150 (7%)]\tLoss: 1.150718\n",
      "Train Epoch: 25 [10/150 (7%)]\tLoss: 1.020408\n",
      "Train Epoch: 26 [10/150 (7%)]\tLoss: 1.093411\n",
      "Train Epoch: 27 [10/150 (7%)]\tLoss: 1.016163\n",
      "Train Epoch: 28 [10/150 (7%)]\tLoss: 1.092652\n",
      "Train Epoch: 29 [10/150 (7%)]\tLoss: 1.081738\n",
      "Train Epoch: 30 [10/150 (7%)]\tLoss: 1.164790\n",
      "Train Epoch: 31 [10/150 (7%)]\tLoss: 1.288264\n",
      "Train Epoch: 32 [10/150 (7%)]\tLoss: 1.152188\n",
      "Train Epoch: 33 [10/150 (7%)]\tLoss: 1.223460\n",
      "Train Epoch: 34 [10/150 (7%)]\tLoss: 1.139751\n",
      "Train Epoch: 35 [10/150 (7%)]\tLoss: 0.960695\n",
      "Train Epoch: 36 [10/150 (7%)]\tLoss: 1.235268\n",
      "Train Epoch: 37 [10/150 (7%)]\tLoss: 1.158565\n",
      "Train Epoch: 38 [10/150 (7%)]\tLoss: 1.089183\n",
      "Train Epoch: 39 [10/150 (7%)]\tLoss: 1.139929\n",
      "Train Epoch: 40 [10/150 (7%)]\tLoss: 1.219665\n",
      "Train Epoch: 41 [10/150 (7%)]\tLoss: 0.950111\n",
      "Train Epoch: 42 [10/150 (7%)]\tLoss: 1.155121\n",
      "Train Epoch: 43 [10/150 (7%)]\tLoss: 1.079999\n",
      "Train Epoch: 44 [10/150 (7%)]\tLoss: 1.143373\n",
      "Train Epoch: 45 [10/150 (7%)]\tLoss: 1.209685\n",
      "Train Epoch: 46 [10/150 (7%)]\tLoss: 1.217738\n",
      "Train Epoch: 47 [10/150 (7%)]\tLoss: 1.215727\n",
      "Train Epoch: 48 [10/150 (7%)]\tLoss: 1.087870\n",
      "Train Epoch: 49 [10/150 (7%)]\tLoss: 1.231718\n",
      "Train Epoch: 50 [10/150 (7%)]\tLoss: 0.880074\n",
      "Train Epoch: 51 [10/150 (7%)]\tLoss: 1.355074\n",
      "Train Epoch: 52 [10/150 (7%)]\tLoss: 1.151208\n",
      "Train Epoch: 53 [10/150 (7%)]\tLoss: 1.077018\n",
      "Train Epoch: 54 [10/150 (7%)]\tLoss: 1.158279\n",
      "Train Epoch: 55 [10/150 (7%)]\tLoss: 1.096971\n",
      "Train Epoch: 56 [10/150 (7%)]\tLoss: 1.216427\n",
      "Train Epoch: 57 [10/150 (7%)]\tLoss: 1.086365\n",
      "Train Epoch: 58 [10/150 (7%)]\tLoss: 1.216338\n",
      "Train Epoch: 59 [10/150 (7%)]\tLoss: 1.288644\n",
      "Train Epoch: 60 [10/150 (7%)]\tLoss: 1.157184\n",
      "Train Epoch: 61 [10/150 (7%)]\tLoss: 1.217094\n",
      "Train Epoch: 62 [10/150 (7%)]\tLoss: 1.214288\n",
      "Train Epoch: 63 [10/150 (7%)]\tLoss: 1.152937\n",
      "Train Epoch: 64 [10/150 (7%)]\tLoss: 1.220309\n",
      "Train Epoch: 65 [10/150 (7%)]\tLoss: 1.135264\n",
      "Train Epoch: 66 [10/150 (7%)]\tLoss: 1.095546\n",
      "Train Epoch: 67 [10/150 (7%)]\tLoss: 1.011676\n",
      "Train Epoch: 68 [10/150 (7%)]\tLoss: 1.021740\n",
      "Train Epoch: 69 [10/150 (7%)]\tLoss: 1.272748\n",
      "Train Epoch: 70 [10/150 (7%)]\tLoss: 1.280737\n",
      "Train Epoch: 71 [10/150 (7%)]\tLoss: 1.086630\n",
      "Train Epoch: 72 [10/150 (7%)]\tLoss: 1.018987\n",
      "Train Epoch: 73 [10/150 (7%)]\tLoss: 1.025217\n",
      "Train Epoch: 74 [10/150 (7%)]\tLoss: 1.092615\n",
      "Train Epoch: 75 [10/150 (7%)]\tLoss: 1.153641\n",
      "Train Epoch: 76 [10/150 (7%)]\tLoss: 1.215181\n",
      "Train Epoch: 77 [10/150 (7%)]\tLoss: 1.212525\n",
      "Train Epoch: 78 [10/150 (7%)]\tLoss: 1.141538\n",
      "Train Epoch: 79 [10/150 (7%)]\tLoss: 1.073137\n",
      "Train Epoch: 80 [10/150 (7%)]\tLoss: 1.027121\n",
      "Train Epoch: 81 [10/150 (7%)]\tLoss: 1.021277\n",
      "Train Epoch: 82 [10/150 (7%)]\tLoss: 1.082193\n",
      "Train Epoch: 83 [10/150 (7%)]\tLoss: 1.282941\n",
      "Train Epoch: 84 [10/150 (7%)]\tLoss: 1.223025\n",
      "Train Epoch: 85 [10/150 (7%)]\tLoss: 1.005173\n",
      "Train Epoch: 86 [10/150 (7%)]\tLoss: 1.146812\n",
      "Train Epoch: 87 [10/150 (7%)]\tLoss: 1.223638\n",
      "Train Epoch: 88 [10/150 (7%)]\tLoss: 1.346367\n",
      "Train Epoch: 89 [10/150 (7%)]\tLoss: 1.155090\n",
      "Train Epoch: 90 [10/150 (7%)]\tLoss: 1.285674\n",
      "Train Epoch: 91 [10/150 (7%)]\tLoss: 1.214311\n",
      "Train Epoch: 92 [10/150 (7%)]\tLoss: 0.956456\n",
      "Train Epoch: 93 [10/150 (7%)]\tLoss: 1.352105\n",
      "Train Epoch: 94 [10/150 (7%)]\tLoss: 1.150028\n",
      "Train Epoch: 95 [10/150 (7%)]\tLoss: 1.079360\n",
      "Train Epoch: 96 [10/150 (7%)]\tLoss: 1.277725\n",
      "Train Epoch: 97 [10/150 (7%)]\tLoss: 1.012155\n",
      "Train Epoch: 98 [10/150 (7%)]\tLoss: 1.223072\n",
      "Train Epoch: 99 [10/150 (7%)]\tLoss: 1.217398\n",
      "Train Epoch: 100 [10/150 (7%)]\tLoss: 1.139417\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100 + 1):\n",
    "    train(Xc,y,epoch)\n",
    "    #test(Xc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
