{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocodificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer um jupyter notebook com autocodificador, a exemplo do exemplo mostrado em aula usando o Keras:\n",
    "\n",
    "- https://t-adessowiki.fee.unicamp.br/user/aluno/notebooks/ia368z_2s2017/ia368z/deliver-keras/DenseAutoencoder.ipynb\n",
    "\n",
    "Com as seguintes modificações:\n",
    "\n",
    "- Utilizar o PyTorch\n",
    "- Se possível, utilizar um autocodificador convolucional, porém não é imperativo\n",
    "- Uma vez que o autocodificador seja treinado e o espaço bidimensional seja mostrado dos códigos dos dígitos da base de dados NMIST, gerar novas imagens:\n",
    "  - mostrar algumas imagens em pontos quaisquer neste espaço bidimensional\n",
    "  - dados dois pontos neste espaço, mostrar várias imagens, começando no ponto 1 e caminhando numa reta que une os dois pontos, gerar imagens associadas a pontos igualmente espaçados nessa reta, até chegar ao segundo ponto. É se esperar que os números gerados sejam interpolados entre eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento do MNIST dataset (já está no disco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados MNIST está disponível em: http://yann.lecun.com/exdb/mnist/\n",
    "Consiste de 60 mil amostras de treinamento e 10 mil amostras de teste. Cada amostra é uma pequena imagem em níveis de cinza de tamanho 28x28 pixels com valores entre 0 e 255.\n",
    "\n",
    "A base já está carregada e disponível em /data/datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torchvision.datasets.MNIST('/data/datasets/mnist_pytorch/', train=True)\n",
    "data_test  = torchvision.datasets.MNIST('/data/datasets/mnist_pytorch/', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.train_data.shape, data_test.test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando com pouquíssimos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    n_train_samples = 1000\n",
    "    n_test_samples = 500\n",
    "    data_train.train_data = data_train.train_data[:n_train_samples]\n",
    "    data_test.test_data   = data_test.test_data[:n_test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 28, 28]), torch.Size([500, 28, 28]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.train_data.shape, data_test.test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando algumas amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTF\nHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1\nXShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8P\nlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f\n4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE5E93405F8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYf\nX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYG\nhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45\nvx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/Fw\nCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE5E906CCF8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn4\n7ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl\n9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPp\nBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE5E906CDD8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA70lEQVR4nMXQsWoCQRQF0JvBNNrG\ndjcgIR8gJJVrqRZCfkMkgn+QlEIIAcHaHzClVWySJkmXSgxqIbKCbcrLxRSbNe7M2uqr7syZefAe\ncOTK9fTp78MLUs2ds9nJ+b71OPMfWzdXAALz9ZrSVCQpclp0bbiRpPVckmPlmUh268Ed2bDsfEVx\n2skCfsif9qkzxcsZAOCWYsHGDy+K/nuM2zmNuV5E6cQYc5/4+UDG0W07iTFfXlGhl45PJGelKGeQ\nrOElgPFb8vJbqtWW0kYpG2qT8W7ZtdEP/zAcFbI2IniMsOkIAKD6zEGl6qXjweoXXfV/5XmKZEMA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE5E906CE10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= 0\n"
     ]
    }
   ],
   "source": [
    "for k,(x,y) in enumerate(data_test):\n",
    "    display.display(x)\n",
    "    print('label=',y)\n",
    "    if k > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
